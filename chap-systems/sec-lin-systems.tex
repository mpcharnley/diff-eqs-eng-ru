\section{Linear systems of ODEs}
\label{linsystems:section}

\LAtt{3.3}

\LO{
\item Use proper terminiology when discussing linear systems of differential equations and their solutions,
\item Determine whether a set of functions is linearly independent, and
\item Understand how the theory of non-homogeneous linear systems relates to the theory of non-linear equations.
}

In order to get into the details of how to talk about and deal with linear systems of differential equations, we first need to talk about matrix- or vector-valued functions.  
Such a function is
just a matrix or vector whose entries depend on some variable.
If $t$ is the independent variable, we write a
\emph{\myindex{vector-valued function}}
$\vec{x}(t)$ as
\begin{equation*}
\vec{x}(t) = \begin{bmatrix}
x_1(t) \\
x_2(t) \\
\vdots \\
x_n(t)
\end{bmatrix} .
\end{equation*}
Similarly a \emph{\myindex{matrix-valued function}} $A(t)$ is
\begin{equation*}
A(t) =
\begin{bmatrix}
a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t) \\
a_{21}(t) & a_{22}(t) & \cdots & a_{2n}(t) \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1}(t) & a_{n2}(t) & \cdots & a_{nn}(t)
\end{bmatrix} .
\end{equation*}
As long as the addition of vectors is defined, we can add vector-valued functions, and as long as the addition and multiplication of matrices are defined (like if they are the right size) we can multiply matrix-valued functions. In addition, the derivative $A'(t)$ or $\frac{dA}{dt}$ is
just the matrix-valued function
whose $ij^{\text{th}}$ entry is $a_{ij}'(t)$. We used this idea previously when talking about how to write first order systems from higher order equations in \sectionref{sec:introtosys}.

Rules of differentiation of matrix-valued functions
are similar to rules for normal
functions.  Let $A(t)$ and $B(t)$ be matrix-valued
functions.  Let $c$ a scalar and let $C$ be a constant matrix.
Then
\begin{align*}
\bigl(A(t)+B(t)\bigr)' & = A'(t) + B'(t), \\
\bigl(A(t)B(t)\bigr)' & = A'(t)B(t) + A(t)B'(t), \\
\bigl(cA(t)\bigr)' & = cA'(t), \\
\bigl(CA(t)\bigr)' & = CA'(t), \\
\bigl(A(t)\,C\bigr)' & = A'(t)\,C .
\end{align*}
Note the order of the multiplication in the last two expressions because matrix multiplication is not commutative.

A \emph{\myindex{first order linear system of ODEs}} is a system that can be
written as the vector equation
\begin{equation*}
{\vec{x}}'(t) = P(t)\vec{x}(t) + \vec{f}(t),
\end{equation*}
where $P(t)$ is a matrix-valued function,
and $\vec{x}(t)$ and $\vec{f}(t)$ are vector-valued functions.
We will often suppress the dependence on $t$ and
only write ${\vec{x}}' = P\vec{x} + \vec{f}$.  A solution of
the system is a vector-valued function
$\vec{x}$ satisfying the vector equation.

For example, the equations
\begin{align*}
x_1' &= 2t x_1 + e^t x_2 + t^2 , \\
x_2' &= \frac{x_1}{t} -x_2 + e^t ,
\end{align*}
can be written as
\begin{equation*}
{\vec{x}}' = 
\begin{bmatrix}
2t & e^t \\
\nicefrac{1}{t} & -1
\end{bmatrix}
\vec{x}
+
\begin{bmatrix}
t^2 \\
e^t
\end{bmatrix} .
\end{equation*}

We will mostly concentrate on equations that are not just linear, but are in
fact \emph{\myindex{constant coefficient}} equations.  That is, the matrix $P$ will
be constant; it will not depend on $t$.

\medskip

When $\vec{f} = \vec{0}$ (the zero vector), then we say the system is
\emph{homogeneous\index{homogeneous system}}.
For homogeneous linear systems we have the
principle of superposition, just like for single homogeneous equations.

\begin{theorem1}{Superposition}\index{superposition}
Let
${\vec{x}}' = P\vec{x}$ be a linear homogeneous system of ODEs.  Suppose
that $\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n$ are $n$ solutions of the equation
and $c_1,c_2,\ldots,c_n$ are any constants, then
\begin{equation} \label{syshom:eq1}
\vec{x} = c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n \vec{x}_n ,
\end{equation}
is also a solution.
Furthermore, if this is a system of $n$ equations ($P$ is $n\times n$), and
$\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n$ are linearly independent, then every
solution  $\vec{x}$ can be written as \eqref{syshom:eq1}.
\end{theorem1}

Linear independence for vector-valued functions is the same idea as
for normal functions.
The vector-valued functions
$\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n$ are
linearly independent
\index{linearly independent!for vector-valued functions}
if the only way to satisfy the equation
\begin{equation*}
c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n \vec{x}_n  = \vec{0}
\end{equation*}
is by choosing the parameters $c_1 = c_2 = \cdots = c_n = 0$, where the equation
must hold for all $t$.

\begin{example} Determine if the sets $S_1 = \Bigl\{ \vec{x}_1 = \Bigl[ \begin{smallmatrix} t^2 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_2 = \Bigl[ \begin{smallmatrix} 0 \\ 1+t \end{smallmatrix} \Bigr],\ \vec{x}_3 = \Bigl[ \begin{smallmatrix} -t^2 \\ 1 \end{smallmatrix} \Bigr] \Bigr\}$ and $S_2 = \Bigl\{ \vec{x}_1 = \Bigl[ \begin{smallmatrix} t^2 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_2 = \Bigl[ \begin{smallmatrix} 0 \\ t \end{smallmatrix} \Bigr],\ \vec{x}_3 = \Bigl[ \begin{smallmatrix} -t^2 \\ 1 \end{smallmatrix}
\Bigr]\Bigr\}$ are linearly independent.
\end{example}

\begin{exampleSol}
The vector functions in $S_1$ are linearly dependent because
$\vec{x}_1 + \vec{x}_3 = \vec{x}_2$, and this holds for all $t$.  So $c_1 =
1$, $c_2 = -1$, and $c_3 = 1$ above will work.

On the other hand, the vector functions in $S_2$ are linearly independent, even though this is only a slight change from $S_1$.
First write
$c_1 \vec{x}_1 + c_2 \vec{x}_2 + c_3 \vec{x}_3  = \vec{0}$ and note that it
has to hold for all $t$.  We get that
\begin{equation*}
c_1 \vec{x}_1 + c_2 \vec{x}_2 + c_3 \vec{x}_3
=
\begin{bmatrix}
c_1 t^2 - c_3 t^2
\\
c_1 t + c_2 t + c_3 
\end{bmatrix}
=
\begin{bmatrix}
0
\\
0
\end{bmatrix} .
\end{equation*}
In other words
$c_1 t^2 - c_3 t^2 = 0$ and
$c_1 t + c_2 t + c_3 = 0$.
If we set $t = 0$, then the second equation becomes $c_3 = 0$.  But then
the first equation becomes
$c_1 t^2 = 0$ for all $t$ and so $c_1 = 0$.  Thus the second equation
is just $c_2 t = 0$, which means $c_2 = 0$.  So $c_1 = c_2 = c_3 = 0$
is the only solution and $\vec{x}_1$,
$\vec{x}_2$, and $\vec{x}_3$ are linearly independent.
\end{exampleSol}

The linear combination $c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n
\vec{x}_n$ could always be also as
\begin{equation*}
X(t)\,\vec{c} ,
\end{equation*}
where $X(t)$ is the matrix with columns $\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_n$,
and $\vec{c}$ is the column vector with entries $c_1, c_2, \ldots, c_n$. This is similar to the way that we could write linear combinations of vectors by putting them into a matrix, including how we talked about rank in \sectionref{subspaces:section}.
Assuming that $\vec{x}_1,\vec{x}_2,\ldots,\vec{x}_n$ are linearly
independent and solutions to a given system of differential equations,
the matrix-valued function $X(t)$ is called a \emph{\myindex{fundamental matrix}},
or a \emph{\myindex{fundamental matrix solution}}.

\medskip

To solve nonhomogeneous first order linear systems, we use the same
technique as we applied to solve single linear nonhomogeneous equations.

\begin{theorem1}{}
Let
${\vec{x}}' = P\vec{x} + \vec{f}$ be a linear system of ODEs.
Suppose $\vec{x}_p$ is one particular solution.  Then every solution
can be written as
\begin{equation*}
\vec{x} = \vec{x}_c + \vec{x}_p ,
\end{equation*}
where $\vec{x}_c$ is a solution to the
\myindex{associated homogeneous equation}
(${\vec{x}}' = P\vec{x}$).
\end{theorem1}

The procedure for systems is the same as for single equations.
We find a particular solution
to the nonhomogeneous equation, then we find the general solution to the
associated homogeneous equation, and finally we add the two together.

\medskip

Alright, suppose you have found the general solution of
${\vec{x}}' = P\vec{x} + \vec{f}$.  Next suppose you are given an
initial condition of the form
\begin{equation*}
\vec{x}(t_0) = \vec{b}
\end{equation*}
for some fixed $t_0$ and a constant
vector $\vec{b}$.  Let $X(t)$ be a fundamental matrix solution
of the associated homogeneous equation
(i.e.\ columns of $X(t)$ are solutions).  The general solution can be
written as
\begin{equation*}
\vec{x}(t) = X(t)\,\vec{c} + \vec{x}_p(t).
\end{equation*}
We are seeking a vector 
$\vec{c}$ such that
\begin{equation*}
\vec{b} = \vec{x}(t_0) = X(t_0)\,\vec{c} + \vec{x}_p(t_0).
\end{equation*}
In other words, we are solving for $\vec{c}$ in the nonhomogeneous system of linear equations
\begin{equation*}
X(t_0)\,\vec{c} = \vec{b} - \vec{x}_p(t_0) .
\end{equation*}

\begin{example}
In \sectionref{sec:introtosys} we solved the system
\begin{align*}
x_1' & = x_1 , \\
x_2' & = x_1 - x_2 ,
\end{align*}
with initial conditions $x_1(0) = 1$, $x_2(0) = 2$.
Let us consider this problem in the language of this section.

The system is homogeneous, so $\vec{f}(t) = \vec{0}$.
We write the system and the initial conditions as
\begin{equation*}
{\vec{x}}'
=
\begin{bmatrix}
1 & 0 \\
1 & -1
\end{bmatrix}
\vec{x} ,
\qquad
\vec{x}(0) = 
\begin{bmatrix}
1 \\
2
\end{bmatrix} .
\end{equation*}

We found the general solution is
$x_1 = c_1 e^t $ and
$x_2 = \frac{c_1}{2}e^{t} + c_2e^{-t}$. 
Letting $c_1=1$ and $c_2=0$, we obtain the solution
$\left[ \begin{smallmatrix} e^t \\ (1/2) e^t \end{smallmatrix} \right]$.
Letting $c_1=0$ and $c_2=1$, we obtain
$\left[ \begin{smallmatrix} 0 \\ e^{-t} \end{smallmatrix} \right]$.
These two solutions are linearly independent,
as can be seen by setting
$t=0$, and noting that the resulting constant vectors are
linearly independent.
In matrix notation, a fundamental matrix solution is, therefore,
\begin{equation*}
X(t) = 
\begin{bmatrix}
e^t & 0 \\
\frac{1}{2} e^t & e^{-t}
\end{bmatrix} .
\end{equation*}

To solve the initial value problem we solve for $\vec{c}$ in the equation
\begin{equation*}
X(0)\,\vec{c} = \vec{b} ,
\end{equation*}
or in other words,
\begin{equation*}
\begin{bmatrix}
1 & 0 \\
\frac{1}{2} & 1
\end{bmatrix} 
\vec{c} = 
\begin{bmatrix}
1 \\ 2
\end{bmatrix} .
\end{equation*}
A single elementary row operation shows
$\vec{c} =
\left[ \begin{smallmatrix} 1 \\ 3/2 \end{smallmatrix} \right]$.
Our solution is
\begin{equation*}
\vec{x}(t) = 
X(t)\,\vec{c} = 
\begin{bmatrix}
e^t & 0 \\
\frac{1}{2} e^t & e^{-t}
\end{bmatrix}
\begin{bmatrix}
1 \\ \frac{3}{2}
\end{bmatrix} =
\begin{bmatrix}
e^t \\
\frac{1}{2} e^t + \frac{3}{2} e^{-t}
\end{bmatrix} .
\end{equation*}
This new solution agrees with our previous solution from \sectionref{sec:introtosys}.
\end{example}

\subsection{Exercises}

\begin{exercise}
Write the system $x_1' = 2 x_1 - 3t x_2 + \sin t$,
$x_2' = e^t x_1 + 3 x_2 + \cos t$ in the form
${\vec{x}}' = P(t) \vec{x} + \vec{f}(t)$.
\end{exercise}
\comboSol{%
}
{%
$P(t) = \left[\begin{smallmatrix} 2 & -3t \\ e^t & 3 \end{smallmatrix}\right]$, $\vec{f}(t) = \left[\begin{smallmatrix}  \sin(t)\\ \cos(t) \end{smallmatrix}\right]$
}

\begin{exercise}\ansMark%
Write $x'=3x-y+e^t$, $y'=tx$ in matrix notation.
\end{exercise}
\exsol{%
$\left[ \begin{smallmatrix}
x \\ y
\end{smallmatrix}\right] '
=
\left[ \begin{smallmatrix}
3 & -1 \\
t & 0
\end{smallmatrix}\right]
\left[ \begin{smallmatrix}
x \\ y
\end{smallmatrix}\right]
+
\left[ \begin{smallmatrix}
e^{t} \\ 0
\end{smallmatrix}\right]$
}

\begin{exercise}
Consider the third order differential equation
\begin{equation*}
y''' + (y'+1)^2 = e^y + \sin(t+1).
\end{equation*}
Convert this to a first order system and simplify as much as possible. Can you write this in the form $\vec{x}' = A\vec{x} + \vec{f}$? Why or why not?
\end{exercise}
\comboSol{%
}
{%
No, it is non-linear.
$\vec{u}' = \left[\begin{smallmatrix}  u_2 \\ u_3 \\ e^{u_1} - (u_2 + 1)^2 + \sin(t+1) \end{smallmatrix}\right]$
}

\begin{exercise}
\leavevmode
\begin{tasks}
\task
Verify that the system ${\vec{x}}' =
\left[ \begin{smallmatrix}
1 & 3 \\ 3 & 1
\end{smallmatrix} \right] \vec{x}$ has the two solutions
$\left[ \begin{smallmatrix}
1 \\ 1
\end{smallmatrix} \right] e^{4t}$ and
$\left[ \begin{smallmatrix}
1 \\ -1
\end{smallmatrix} \right] e^{-2t}$.
\task
Write down the general solution.
\task
Write down the general solution in the form $x_1 = ?$, $x_2 = ?$
(i.e.\ write down a formula for each element of the solution).
\end{tasks}
\end{exercise}
\comboSol{%
}
{%
b)~ $C_1\left[\begin{smallmatrix} 1 \\ 1 \end{smallmatrix}\right]e^{4t} + C_2\left[\begin{smallmatrix} 1 \\ -1 \end{smallmatrix}\right]e^{-2t}$ \\ c)~$x_1(t) = C_1e^{4t} + C_2e^{-2t},\ x_2(t) = C_1e^{4t} - C_2e^{-2t}$
}

\begin{exercise}
Verify that
$\left[ \begin{smallmatrix}
1 \\ 1
\end{smallmatrix} \right] e^{t}$ and
$\left[ \begin{smallmatrix}
1 \\ -1
\end{smallmatrix} \right] e^{t}$ 
are linearly independent.  Hint: Just plug in $t=0$.
\end{exercise}
\comboSol{%
}
{%
Yes
}


\begin{exercise}\ansMark%
Are
$\left[ \begin{smallmatrix}
e^{2t} \\ e^t
\end{smallmatrix}\right]$
and
$\left[ \begin{smallmatrix}
e^{t} \\ e^{2t}
\end{smallmatrix}\right]$
linearly independent?  Justify.
\end{exercise}
\exsol{%
Yes.
}

\begin{exercise}
Verify that
$\left[ \begin{smallmatrix}
1 \\ 1 \\ 0
\end{smallmatrix} \right] e^{t}$ and
$\left[ \begin{smallmatrix}
1 \\ -1 \\ 1
\end{smallmatrix} \right] e^{t}$ 
and
$\left[ \begin{smallmatrix}
1 \\ -1 \\ 1
\end{smallmatrix} \right] e^{2t}$ 
are linearly independent.  Hint: You must be a bit more tricky than in the
previous exercises.
\end{exercise}
\comboSol{%
}
{%
Yes. Write this out as three equations that all must equal zero, see what combining the first two gets you, then go from there.
}

\begin{exercise}\ansMark%
Are
$\left[ \begin{smallmatrix}
\cosh(t) \\ 1
\end{smallmatrix}\right]$,
$\left[ \begin{smallmatrix}
e^{t} \\ 1
\end{smallmatrix}\right]$,
and
$\left[ \begin{smallmatrix}
e^{-t} \\ 1
\end{smallmatrix}\right]$
linearly independent?  Justify.
\end{exercise}
\exsol{%
No.
$2 \left[ \begin{smallmatrix}
\cosh(t) \\ 1
\end{smallmatrix}\right] - 
\left[ \begin{smallmatrix}
e^{t} \\ 1
\end{smallmatrix}\right]
-
\left[ \begin{smallmatrix}
e^{-t} \\ 1
\end{smallmatrix}\right] = \vec{0}$
}

\begin{exercise}
Verify that
$\left[ \begin{smallmatrix}
t \\ t^2
\end{smallmatrix} \right]$ and
$\left[ \begin{smallmatrix}
t^3 \\ t^4
\end{smallmatrix} \right]$ 
are linearly independent.
\end{exercise}
\comboSol{%
}
{%
Yes, write out the equations and see when they can be zero.
}

\begin{exercise}
Take the system $x_1' + x_2' = x_1$,
$x_1' - x_2' = x_2$.
\begin{tasks}
\task Write it in the form
$A {\vec{x}}' = B \vec{x}$ for matrices $A$ and $B$.
\task Compute $A^{-1}$ and use that to write the system in the form
${\vec{x}}' = P \vec{x}$.
\end{tasks}
\end{exercise}
\comboSol{%
}
{%
$A = \left[\begin{smallmatrix}  1 & 1 \\ 1 & -1 \end{smallmatrix}\right]$, $B = \left[\begin{smallmatrix} 1 & 0 \\ 0 & 1 \end{smallmatrix}\right]$, $P = \left[\begin{smallmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \end{smallmatrix}\right]$
}

\begin{exercise}\ansMark%
\leavevmode
\begin{tasks}
\task Write $x_1'=2tx_2$, $x_2'=2tx_2$ in matrix notation.
\task Solve and write
the solution in matrix notation.
\end{tasks}
\end{exercise}
\exsol{%
a)
$\vec{x}\,'
=
\left[ \begin{smallmatrix}
0 & 2t \\
0 & 2t
\end{smallmatrix}\right]
\vec{x}$
\quad
b)
$\vec{x}
=
\left[ \begin{smallmatrix}
C_2 e^{t^2} + C_1 \\
C_2 e^{t^2}
\end{smallmatrix}\right]$
}

\setcounter{exercise}{100}

